# -*- coding: utf-8 -*-
"""Copy of Maml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11t3ZuaNlu-XwBdiu2XKRSEstiwlTEbSJ
"""

!unzip "drive/MyDrive/data.zip"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Load the train dataset from CSV
train_df = pd.read_csv('MITBIH_TRAIN.csv')

# Extract the input data (X_train) and class labels (y_train) from the train dataset
X_train = train_df.iloc[:, :-1].values
y_train = train_df.iloc[:, -1].values
X_train = np.expand_dims(X_train, axis=-1)
# Load the test dataset from CSV
test_df = pd.read_csv('MITBIH_TEST.csv')

# Extract the input data (X_test) and class labels (y_test) from the test dataset
X_test = test_df.iloc[:, :-1].values
y_test = test_df.iloc[:, -1].values
X_test = np.expand_dims(X_test, axis=-1)
# Print the shape of the data arrays
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

#1st Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import numpy as np

# model
classification_input_shape = (187, 1)
classification_model = Sequential()
classification_model.add(Conv1D(32, 3, activation='relu', input_shape=classification_input_shape))
classification_model.add(MaxPooling1D(2))
classification_model.add(Flatten())
classification_model.add(Dense(16, activation='relu'))
classification_model.add(Dropout(0.2))
classification_model.add(Dense(5, activation='softmax'))
classification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

#classification_model.fit(xtrain, ytrain, epochs=50, batch_size=5, validation_data=(xtest, ytest))

#2nd Dataset
import pandas as pd
from sklearn.model_selection import train_test_split

# Read the CSV file into a DataFrame
df = pd.read_csv('normal_dataset.csv')

# Separate features (X) and class labels (y)
df=df.values
X = df[:, :-1]  # Exclude the last column
#X=X.fillna(0)
y = df[:, -1]   # Last column as the class labels

# Split the dataset into train and test sets, maintaining the class ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

# Display the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

classification_model.save("initial_2.h5")

classification_model.save("pretrained.h5")

#Model Agnostic Meta-Learning
from tensorflow.keras.utils import to_categorical
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from scipy.special import kl_div


# Set the number of jobs and tasks
J = 100
I = 30

# Define the model architecture
classification_input_shape = (187, 1)
classification_model = Sequential()
classification_model.add(Conv1D(32, 3, activation='relu', input_shape=classification_input_shape))
classification_model.add(MaxPooling1D(2))
classification_model.add(Flatten())
classification_model.add(Dense(16, activation='relu'))
classification_model.add(Dropout(0.2))
classification_model.add(Dense(2, activation='softmax'))
classification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
# initial_weights = classification_model.get_weights()

# ones_weights = [np.ones_like(w) for w in initial_weights]

# classification_model.set_weights(ones_weights)

classification_model.load_weights("initial_2.h5")
theta0=classification_model.get_weights()

# Set the number of samples to be selected from X_train for each task
samples_per_task = 25

# Set the number of support samples and query samples
support_samples = 5
query_samples = samples_per_task - support_samples

# Set the number of gradient descent steps
num_steps = J

# List to store the test losses
test_losses = []

# Meta-training loop
for j in range(1, J+1):
    #classification_model.set_weights(theta0)
    test_losses3=[]
    if(j==50):
      #classification_model.save("j50_2_5shot.h5")
      print(1)

    gradients_sum = [np.zeros_like(w) for w in classification_model.get_weights()]
    for i in range(1, I+1):


       # Sample samples_per_task samples from X_train
        task_samples_indices = []
        task_samples_indices_1 = []
        for class_label in range(2):
            class_indices = np.where(y_train == class_label)[0]
            selected_indices = np.random.choice(class_indices, 20, replace=False)
            selected_indices_1 = np.random.choice(class_indices, 1, replace=False)
            task_samples_indices.extend(selected_indices.tolist())
            task_samples_indices_1.extend(selected_indices_1.tolist())
        query_set = X_train[task_samples_indices]
        query_labels = y_train[task_samples_indices]

        support_set=X_train[task_samples_indices_1]
        support_labels=y_train[task_samples_indices_1]
        support_labels = to_categorical(support_labels,num_classes=2)
        query_labels = to_categorical(query_labels,num_classes=2)

        if(j==1 and i==1):
          print(np.shape(support_set),np.shape(query_set))








        #to_categorical(y_test,num_classes=5)

        # Fit the model with theta0 using the support set
        classification_model.set_weights(theta0)
        classification_model.fit(support_set, support_labels, epochs=8
                                 , batch_size=10, verbose=0)
        with tf.GradientTape() as tape:
            predictions = classification_model(query_set, training=False)
            loss_value = tf.keras.losses.categorical_crossentropy(query_labels, predictions)

        gradients = tape.gradient(loss_value, classification_model.trainable_weights)

        # Store the gradients in the test loss array
        test_losses.append(gradients)
        test_loss34 = classification_model.evaluate(query_set, query_labels, verbose=0)[0]
        accuracy = classification_model.evaluate(query_set, query_labels, verbose=0)[1]
        print(accuracy)
        test_losses3.append(test_loss34)

        # Update the sum of gradients
        gradients_sum = [sum_gradients + gradient.numpy() for sum_gradients, gradient in zip(gradients_sum, gradients)]

    # Calculate the average gradients
    average_gradients = [gradients / I for gradients in gradients_sum]

    # Update theta0 with the average gradients
    k=0
    # Update theta0 with the average gradients
    k = 0
    lr_inner=0.005

    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_inner)
    variables = classification_model.trainable_variables
    optimizer.build(variables)

    for layer in classification_model.layers:
        if isinstance(layer, tf.keras.layers.Conv1D) or isinstance(layer, tf.keras.layers.Dense):
            gradients_to_apply = [tf.multiply(lr_inner, gradient) for gradient in average_gradients[k:k+2]]
            variables = [layer.kernel, layer.bias]
            optimizer.apply_gradients(zip(gradients_to_apply, variables))
            k += 2


    #theta0 = [w - 0.005 * gradient for w, gradient in zip(theta0, average_gradients)]
    theta0=classification_model.get_weights()
    for layer in classification_model.layers:
      layer.trainable = True

    # Print the progress
    average_test_loss = np.mean(test_losses3[-I:])
    test_losses=[]
    print(f"Job {j}/{J} completed. Average test loss: {average_test_loss}")

# The final theta will be theta30
final_theta = theta0

classification_model.save("j100_2_5shot_ep8.h5")

#classification_model.set_weights(final_theta)
classification_model.save("10epochext.h5")

theta0=classification_model1.get_weights()

from tensorflow.keras.utils import to_categorical
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from scipy.special import kl_div

classification_input_shape = (187, 1)
classification_model = Sequential()
classification_model.add(Conv1D(32, 3, activation='relu', input_shape=classification_input_shape))
classification_model.add(MaxPooling1D(2))
classification_model.add(Flatten())
classification_model.add(Dense(16, activation='relu'))
classification_model.add(Dropout(0.2))
classification_model.add(Dense(2, activation='softmax'))
classification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

#Meta-testing
from numpy.random import seed
# Set the number of tasks for meta-test
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
import statistics
I = 900
kit=[]
var=[]
# Set the number of samples to be selected from X_test for each task
samples_per_task = 25

# Set the number of support samples and query samples
support_samples = 5
query_samples = samples_per_task - support_samples

# List to store the accuracies
accuracies,precisions,recalls,f1ss = [],[],[],[]

# Meta-test loop
for i in range(1, I+1):
    task_samples_indices = []
    task_samples_indices_1 = []
    for class_label in range(2):
        class_indices = np.where(y_test == class_label)[0]
        class_indices_1 = np.where(y_train == class_label)[0]
        selected_indices = np.random.choice(class_indices, 15, replace=False)
        selected_indices_1 = np.random.choice(class_indices_1, 5, replace=False)
        task_samples_indices.extend(selected_indices.tolist())
        task_samples_indices_1.extend(selected_indices_1.tolist())




    query_set = X_test[task_samples_indices]
    query_labels = y_test[task_samples_indices]
    tem=query_labels
    #print(query_labels)
    support_set=X_train[task_samples_indices_1]
    support_labels=y_train[task_samples_indices_1]
    #print(support_labels)
    support_labels = to_categorical(support_labels,num_classes=2)
    #print(support_labels)
    query_labels = to_categorical(query_labels,num_classes=2)

    # Assuming you have the support set and query set as numpy arrays

    # Initialize the model with the final_theta from meta-training
    classification_model.load_weights("best_model.h5")

    # Fit the model on the support set
    classification_model.fit(support_set, support_labels, epochs=20, batch_size=5, verbose=0)


# Assuming your classification model is already defined and trained

# Make predictions on the query set
    predictions = classification_model.predict(query_set)
    #print(predictions)
    predicted_labels = np.argmax(predictions, axis=1)  # Assuming one-hot encoded labels
    #query_labels = np.argmax(query_labels, axis=1)

    #Calculate the confusion matrix
    #print(query_labels)
    cm = confusion_matrix(tem, predicted_labels)
    #print("Confusion Matrix:")
    #print(cm)
    class_wise_accuracy = cm.diagonal() / cm.sum(axis=1)
    class_wise_accuracy = np.nan_to_num(class_wise_accuracy, nan=0)  # Replace NaN values with 0
    print("Class-wise Accuracy:")
    for i, acc in enumerate(class_wise_accuracy):
        print(f"Class {i}: {acc}")

    accuracy = classification_model.evaluate(query_set, query_labels, verbose=0)[1]
    print(accuracy)
    var.append(100*accuracy)
    if(len(var)>=2):
      variance = statistics.variance(var)
      print("var",variance)

    # Evaluate the accuracy on the query set
    #accuracy = classification_model.evaluate(query_set, query_labels, verbose=0)[1]

    predictions = classification_model.predict(query_set)  # Assuming you have obtained the predictions
    true_labels = query_labels  # Assuming you have the true labels


    max_values = np.max(predictions, axis=1)  # Get the maximum values for each row
    predictions = np.where(predictions == max_values.reshape(-1, 1), 1, 0)  # Replace max values with 1, rest with 0
    #print(predictions)
    # threshold = 0.5  # Set the threshold value
    # predictions = np.where(predictions >= threshold, 1, 0)
    predictions = np.argmax(predictions, axis=1)
    #print(predictions)
    c=[]
    true_labels=np.argmax(true_labels, axis=1)
    #print(true_labels)

    # Calculate accuracy
    accuracyb = accuracy_score(true_labels, predictions)

    # Calculate precision
    precision = precision_score(true_labels, predictions, average='macro')

    # Calculate recall
    recall = recall_score(true_labels, predictions, average='macro')

    # Calculate F1 score
    f1 = f1_score(true_labels, predictions, average='macro')

    # Print the metrics
    # print("Accuracy:", accuracyb)
    # print("Precision:", precision)
    # print("Recall:", recall)
    # print("F1 score:", f1)


    #print(accuracy)
    accuracies.append(accuracyb)
    precisions.append(precision)
    recalls.append(recall)
    f1ss.append(f1)
    predictions = classification_model.predict(query_set)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = np.argmax(query_labels, axis=1)

    # Compute the accuracy for each class
    # class_accuracy = []
    # for class_label in range(2):
    #     class_indices = np.where(true_labels == class_label)[0]
    #     class_predictions = predicted_labels[class_indices]
    #     class_accuracy.append(np.mean(class_predictions == class_label))

    # # Find the class with the lowest accuracy
    # least_accurate_class = np.argmin(class_accuracy)
    # #print("least",least_accurate_class)

# Calculate the average accuracy
average_accuracy = np.mean(accuracies)
average_precision = np.mean(precisions)
average_recall=np.mean(recalls)
average_f1=np.mean(f1ss)

# Print the average accuracy
print(f"Average accuracy over {I} tasks: {average_accuracy}")
print(f"Average precision over {I} tasks: {average_precision}")
print(f"Average recall over {I} tasks: {average_recall}")
print(f"Average f1 over {I} tasks: {average_f1}")