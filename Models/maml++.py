# -*- coding: utf-8 -*-
"""MAML++.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LJlXRtzact2YDCyXJ0h56aktU9V0QY8c
"""

!unzip "drive/MyDrive/data.zip"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Load the train dataset from CSV
train_df = pd.read_csv('drive/MyDrive/mit_bih_new/mit_train.csv', skiprows=1)

# Extract the input data (X_train) and class labels (y_train) from the train dataset
X_train = train_df.iloc[:, :-1].values
y_train = train_df.iloc[:, -1].values
X_train = np.expand_dims(X_train, axis=-1)
# Load the test dataset from CSV
test_df = pd.read_csv('drive/MyDrive/mit_bih_new/mit_test.csv', skiprows=1)

# Extract the input data (X_test) and class labels (y_test) from the test dataset
X_test = test_df.iloc[:, :-1].values
y_test = test_df.iloc[:, -1].values
X_test = np.expand_dims(X_test, axis=-1)
# Print the shape of the data arrays
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

import pandas as pd
from sklearn.model_selection import train_test_split

# Read the CSV file into a DataFrame
df = pd.read_csv('normal_dataset.csv')

# Separate features (X) and class labels (y)
df=df.values
X = df[:, :-1]  # Exclude the last column
#X=X.fillna(0)
y = df[:, -1]   # Last column as the class labels

# Split the dataset into train and test sets, maintaining the class ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)

# Display the shapes of the resulting sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

#MAML++
from tensorflow.keras.utils import to_categorical
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Set the number of jobs and tasks
J = 70
I = 30

# Define the model architecture
classification_input_shape = (280, 1)
classification_model = Sequential()
classification_model.add(Conv1D(32, 3, activation='relu', input_shape=classification_input_shape))
classification_model.add(MaxPooling1D(2))
classification_model.add(Flatten())
classification_model.add(Dense(16, activation='relu'))
classification_model.add(Dropout(0.2))
classification_model.add(Dense(4, activation='softmax'))
classification_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
# initial_weights = classification_model.get_weights()

# ones_weights = [np.ones_like(w) for w in initial_weights]

# classification_model.set_weights(ones_weights)
classification_model.load_weights("good.h5")
theta0=classification_model.get_weights()
first=classification_model.get_weights()

# Set the number of samples to be selected from X_train for each task
samples_per_task = 25

# Set the number of support samples and query samples
support_samples = 5
query_samples = samples_per_task - support_samples

# Set the number of gradient descent steps
num_steps = J

# List to store the test losses
test_losses = []
average_test_loss_1=2
average_test_loss=2
# Meta-training loop
for j in range(1, J+1):
  if(average_test_loss_1>average_test_loss):
    average_test_loss_1=average_test_loss
  classification_model.save("good1.h5")
  #classification_model.set_weights(theta0)
  test_losses3=[]

  gradients_sum = [np.zeros_like(w) for w in theta0]
  for i in range(1, I+1):

      # Sample samples_per_task samples from X_train
      task_samples_indices = []
      task_samples_indices_1 = []
      for class_label in range(4):
          class_indices = np.where(y_train == class_label)[0]
          selected_indices = np.random.choice(class_indices, 20, replace=False)
          selected_indices_1 = np.random.choice(class_indices, 5, replace=False)
          task_samples_indices.extend(selected_indices.tolist())
          task_samples_indices_1.extend(selected_indices_1.tolist())
      query_set = X_train[task_samples_indices]
      query_labels = y_train[task_samples_indices]
      support_set=X_train[task_samples_indices_1]
      support_labels=y_train[task_samples_indices_1]
      support_labels = to_categorical(support_labels,num_classes=4)
      query_labels = to_categorical(query_labels,num_classes=4)

      if(j==1 and i==1):
        print(np.shape(support_set),np.shape(query_set))



      #to_categorical(y_test,num_classes=5)

      # Fit the model with theta0 using the support set
      classification_model.set_weights(theta0)
      #classification_model.fit(support_set, support_labels, epochs=10, batch_size=10, verbose=0)


      weighted_gradients_sum = [np.zeros_like(w) for w in theta0]  # Sum of weighted gradients for each parameter
      weights = a  # Weights for the weighted average
      num_epochs=25
      epoch_gradients_sum = [np.zeros_like(w) for w in theta0]  # Sum of gradients for each epoch

      for epoch in range(1, num_epochs + 1):
          classification_model.fit(support_set, support_labels, epochs=1, batch_size=10, verbose=0)
          with tf.GradientTape() as tape:
              predictions = classification_model(query_set, training=False)
              loss_value = tf.keras.losses.categorical_crossentropy(query_labels, predictions)

          gradients = tape.gradient(loss_value, classification_model.trainable_weights)
          epoch_gradients_sum = [epoch_gradients_sum[i] + (weights[i] * gradients[i].numpy()) for i in range(len(gradients))]

      weighted_gradients = [epoch_gradients_sum[i] / num_epochs for i in range(len(epoch_gradients_sum))]
      weighted_gradients_sum = [weighted_gradients_sum[i] + weighted_gradients[i] for i in range(len(weighted_gradients_sum))]

      k = 0
      #lr_inner=0.01
      # if(j>10):
      #   print("y")
      #   lr_inner=0.007
      # if(j>20):
      #   lr_inner=0.004
      #if(j>30):
      lr_inner=0.002


      optimizer = tf.keras.optimizers.Adam(learning_rate=lr_inner)
      variables = classification_model.trainable_variables
      optimizer.build(variables)

      for layer in classification_model.layers:
          if isinstance(layer, tf.keras.layers.Conv1D) or isinstance(layer, tf.keras.layers.Dense):
              gradients_to_apply = [tf.multiply(lr_inner, gradient) for gradient in weighted_gradients_sum[k:k+2]]
              variables = [layer.kernel, layer.bias]
              optimizer.apply_gradients(zip(gradients_to_apply, variables))
              k += 2
      theta0=classification_model.get_weights()


  #theta0 = [w - 0.005 * gradient for w, gradient in zip(theta0, average_gradients)]

      # Update theta0 with the weighted average gradients
      #theta0 = [w - 0.001 * gradient for w, gradient in zip(theta0, weighted_gradients_sum)]

      # Append the weighted average gradients to test_losses
      test_losses.append(weighted_gradients_sum)



      test_loss34 = classification_model.evaluate(query_set, query_labels, verbose=0)[0]
      test_losses3.append(test_loss34)
      if(j==30):
        #classification_model.save("job30.h5")
        print(1)

  # Print the progress
  average_test_loss = np.mean(test_losses3[-I:])
  test_losses=[]
  print(f"Job {j}/{J} completed. Average test loss: {average_test_loss}")

# The final theta will be theta30
final_theta = theta0

classification_model.save("1SHOTBEST.h5")